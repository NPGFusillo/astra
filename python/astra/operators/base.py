import inspect
import os
import uuid
from time import sleep, time
from airflow.exceptions import AirflowSkipException
from airflow.models import BaseOperator
from datetime import datetime
from subprocess import CalledProcessError

from astra.database.utils import (create_task_instance, serialize_pks_to_path)
from astra.utils import log, get_scratch_dir

from astra.operators.utils import (fulfil_defaults_for_data_model_identifiers, prepare_data)

class AstraOperator(BaseOperator):
    """
    A base operator for performing work on SDSS data products.

    :param spectrum_callback: 
        The name of a function to execute on spectra after they are loaded 
        (e.g., 'astra.tools.continuum.mean').
    :type spectrum_callback: str
    :param spectrum_callback_kwargs: 
        A dictionary of keyword arguments that will be submitted to the `spectrum_callback`.
    :param slurm_kwargs: 
        A dictionary of keyword arguments that will be submitted to the slurm queue.
    :type slurm_kwargs: dict
    :param _data_model_identifiers:
        Directly supply data model identifiers to this operator and *do not* find data model
        identifiers for the given DAG execution context. Only use this if you know what you
        are doing. Using this argument will override the default behaviour of the operators,
        and will make the operator only process the data that is given by `_data_model_identifiers`,
        regardless of the execution date. This means the operator will process the same set
        of data every time it is triggered! This argument should only be used for testing.
        An example use of this argument might be:

        >> _data_model_identifiers = [
        >>     {
        >>         "obj": "2M06343180+0546387", 
        >>         "telescope": "apo25m",
        >>         "apstar": "stars",
        >>         "healpix": "88460",
        >>         "apred": "daily",
        >>         "filetype": "ApStar", 
        >>         "release": "sdss5"
        >>     }
        >> ]

    :type _data_model_identifiers:
        An iterable that includes dictionaries that fully define a data model product, or a
        callable function that returns an iterable.
    """

    ui_color = "#CEE8F2"

    def __init__(
        self,
        spectrum_callback=None,
        spectrum_callback_kwargs=None,
        slurm_kwargs=None,
        _data_model_identifiers=None,
        **kwargs
    ):
        super(AstraOperator, self).__init__(**kwargs)
        self.spectrum_callback = spectrum_callback
        self.spectrum_callback_kwargs = spectrum_callback_kwargs
        self.slurm_kwargs = slurm_kwargs
        self._data_model_identifiers = _data_model_identifiers


    def inherited_parameters(
            self, 
            ignore=("self", "args", "kwargs", "slurm_kwargs"),
            ignore_keywords_with_leading_underscores=True,
            ignore_arguments_with_nones_and_default_values=True,
        ):
        """
        Return a dictionary of keyword arguments that should be recorded as parameters for
        task instances generated by this operator. This will include all named keyword
        arguments that were initiated with the class, including those from parent
        classes. 

        :param ignore:
            keyword names to ignore
        :type ignore: tuple of strings
        :param ignore_keywords_with_leading_underscores:
            ignore keywords with leading underscores (e.g., '_data_model_identifiers') (default: True)
        type ignore_keywords_with_leading_underscores: bool
        :param ignore_arguments_with_nones_and_default_values:
            ignore arguments where the value given is the default value (e.g., None).
        :type ignore_arguments_with_nones_and_default_values: bool

        :returns:
            A dictionary of keyword arguments to include as task parameters.
        """

        inherited_parameters = {}
        mros = type(self).mro()
        
        for mro in mros[:1 + mros.index(AstraOperator)]:
            signature = inspect.signature(mro.__init__)
            init_parameters = signature.parameters
            if ignore_keywords_with_leading_underscores:
                init_parameters = [pn for pn in init_parameters if not pn.startswith("_")]
            
            parameters = set(init_parameters).difference(ignore)
            for parameter in parameters:
                if ignore_arguments_with_nones_and_default_values \
                and getattr(self, parameter) is None \
                and signature.parameters[parameter].default == getattr(self, parameter):
                    continue
                inherited_parameters.setdefault(parameter, getattr(self, parameter))

        return inherited_parameters
    

    @property
    def bash_command_line_arguments(self):
        """
        Return the parameters supplied to this task as a string of command line arguments.
        """

        cla = []
        for key, parameter in inspect.signature(self.__init__).parameters.items():
            if key.startswith("_") or key in ("args", "kwargs", "slurm_kwargs"): continue

            value = getattr(self, key)
            if parameter.default is parameter.empty:
                # Supply as an argument.
                cla.append(value)
            else:
                cla.append(f"--{key.replace('_', '-')} {value}")
        
        return " ".join(cla)


    def data_model_identifiers(self, context):
        """ 
        Yield the data model identifiers found by this operator.
        
        :param context:
            The Airflow context dictionary.
        """

        if self._data_model_identifiers is not None:
            log.warning("Using data model identifiers specified by _data_model_identifiers. "
                        "Ignoring the DAG execution context! Use this at your own risk!")

            sources = self._data_model_identifiers
            if callable(sources):
                sources = sources()
            
            yield from fulfil_defaults_for_data_model_identifiers(sources, context)

        else:
            yield from self.query_data_model_identifiers_from_database(context)


    def pre_execute(self, context):
        """
        Create task instances for all the data model identifiers. 
        
        :param context:
            The Airflow context dictionary.
        """

        args = (context["dag"].dag_id, context["task"].task_id, context["run_id"])

        # Get parameters from the parent class initialisation that should also be stored.
        inherited_parameters = self.inherited_parameters()

        pks = []
        for data_model_identifiers in self.data_model_identifiers(context):
            # This order here is important if inherited_parameters['release'] = None,
            # and the release has been inferred from the execution date.
            parameters = { **inherited_parameters, **data_model_identifiers }
            instance = create_task_instance(*args, parameters)
            pks.append(instance.pk)
        
        if not pks:
            raise AirflowSkipException("No data model identifiers found for this time period.")
        
        self.pks = pks
        return None
        

    def prepare_data(self):
        """
        A generator that yields the task instance, the path of the input data product, the 
        spectrum, and the modified spectrum (after applying any spectrum callbacks).
        """
        yield from prepare_data(self.pks)


    def execute(self, context):
        """
        Execute the operator.

        :param context:
            The Airflow DAG execution context.
        """

        if not self.slurm_kwargs and callable(getattr(self, "python_callable", None)):
            # Use Python callable.
            log.info(f"Executing Python callable in {self}")
            self.python_callable(
                self.pks,
                **self.inherited_parameters()
            )

        else:
            # Use bash command. Maybe in Slurm.
            pks_path = serialize_pks_to_path(
                self.pks,
                dir=get_scratch_dir()
            )
            log.info(f"Serialized {len(self.pks)} primary keys to {pks_path}")

            try:
                bash_command = f"{self.bash_command_prefix} {pks_path} {self.bash_command_line_arguments}"
            except:
                log.exception(f"Cannot construct bash command")
                raise
            else:
                log.info(f"Executing bash command:\n{bash_command}")

            if self.slurm_kwargs:
                self.execute_by_slurm(
                    context,
                    bash_command,
                )
            else:
                raise NotImplementedError
            
            # Remove the temporary file.
            os.unlink(pks_path)

        return self.pks
    

    def execute_by_slurm(
        self, 
        context, 
        bash_command,
        poke_interval=60
    ):
        
        uid = str(uuid.uuid4())[:8]
        label = ".".join([
            context["dag"].dag_id,
            context["task"].task_id,
            context["execution_date"].strftime('%Y-%m-%d'),
            # run_id is None if triggered by command line
            uid
        ])

        # It's bad practice to import here, but the slurm package is
        # not easily installable outside of Utah, and is not a "must-have"
        # requirement. 
        from slurm import queue
        
        # TODO: HACK to be able to use local astra installation while in development
        if bash_command.startswith("astra "):
            bash_command = f"/uufs/chpc.utah.edu/common/home/u6020307/.local/bin/astra {bash_command[6:]}"

        slurm_kwargs = (self.slurm_kwargs or dict())

        log.info(f"Submitting Slurm job {label} with command:\n\t{bash_command}\nAnd Slurm keyword arguments: {slurm_kwargs}")        
        q = queue(verbose=True)
        q.create(label=label, **slurm_kwargs)
        q.append(bash_command)
        try:
            q.commit(hard=True, submit=True)
        except CalledProcessError as e:
            log.exception(f"Exception occurred when committing Slurm job with output:\n{e.output}")
            raise

        log.info(f"Slurm job submitted with {q.key} and keywords {slurm_kwargs}")
        log.info(f"\tJob directory: {q.job_dir}")

        stdout_path = os.path.join(q.job_dir, f"{label}_01.o")
        stderr_path = os.path.join(q.job_dir, f"{label}_01.e")    

        # Now we wait until the Slurm job is complete.
        t_submitted, t_started = (time(), None)
        while 100 > q.get_percent_complete():

            sleep(poke_interval)

            t = time() - t_submitted

            if not os.path.exists(stderr_path) and not os.path.exists(stdout_path):
                log.info(f"Waiting on job {q.key} to start (elapsed: {t / 60:.0f} min)")

            else:
                # Check if this is the first time it has started.
                if t_started is None:
                    t_started = time()
                    log.debug(f"Recording job {q.key} as starting at {t_started} (took {t / 60:.0f} min to start)")

                log.info(f"Waiting on job {q.key} to finish (elapsed: {t / 60:.0f} min)")
                # Open last line of stdout path?

                # If this has been going much longer than the walltime, then something went wrong.
                # TODO: Check on the status of the job from Slurm.

        log.info(f"Job {q.key} in {q.job_dir} is complete after {(time() - t_submitted)/60:.0f} minutes.")

        with open(stderr_path, "r", newline="\n") as fp:
            stderr = fp.read()
        log.info(f"Contents of {stderr_path}:\n{stderr}")

        with open(stdout_path, "r", newline="\n") as fp:
            stdout = fp.read()
        log.info(f"Contents of {stdout_path}:\n{stdout}")
        
        # TODO: Better parsing for critical errors.
        if "Error" in stderr.rstrip().split("\n")[-1]:
            raise RuntimeError(f"detected exception at task end-point")

        return None
        


