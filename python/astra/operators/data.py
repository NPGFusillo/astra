
import inspect
from airflow.exceptions import AirflowSkipException

from astra.database.utils import create_task_instance
from astra.operators.base import AstraOperator
from astra.operators.utils import (fulfil_defaults_for_data_model_identifiers, prepare_data)
from astra.utils import log

class DataProductOperator(AstraOperator):

    """
    A base operator for performing work on data products.
    """

    def common_task_parameters(
            self, 
            ignore=("self", "args", "kwargs", "slurm_kwargs"),
            ignore_keywords_with_leading_underscores=True,
            ignore_arguments_with_nones_and_default_values=True,
        ):
        """
        Return a dictionary of keyword arguments that should be recorded as parameters for
        task instances generated by this operator. This will include:
        
            - all named keyword arguments that were initiated with the class, 
            - including those keyword arguments from parent classes,
            - and any parameters passed through `bash_command_kwargs`

        :param ignore:
            keyword names to ignore
        :type ignore: tuple of strings
        :param ignore_keywords_with_leading_underscores:
            ignore keywords with leading underscores (e.g., '_data_model_identifiers') (default: True)
        type ignore_keywords_with_leading_underscores: bool
        :param ignore_arguments_with_nones_and_default_values:
            ignore arguments where the value given is the default value (e.g., None).
        :type ignore_arguments_with_nones_and_default_values: bool

        :returns:
            A dictionary of keyword arguments to include as task parameters.
        """

        common_task_parameters = {}
        mros = type(self).mro()
        
        for mro in mros[:1 + mros.index(AstraOperator)]:
            signature = inspect.signature(mro.__init__)
            init_parameters = signature.parameters
            if ignore_keywords_with_leading_underscores:
                init_parameters = [pn for pn in init_parameters if not pn.startswith("_")]
            
            parameters = set(init_parameters).difference(ignore)
            for parameter in parameters:
                if ignore_arguments_with_nones_and_default_values \
                and getattr(self, parameter) is None \
                and signature.parameters[parameter].default == getattr(self, parameter):
                    continue
                common_task_parameters.setdefault(parameter, getattr(self, parameter))

        return common_task_parameters


    def data_model_identifiers(self, context):
        """ 
        Yield the data model identifiers found by this operator.
        
        :param context:
            The Airflow context dictionary.
        """

        if self._data_model_identifiers is not None:
            log.warning("Using data model identifiers specified by _data_model_identifiers. "
                        "Ignoring the DAG execution context! Use this at your own risk!")

            sources = self._data_model_identifiers
            if callable(sources):
                sources = sources()

            yield from fulfil_defaults_for_data_model_identifiers(sources, context)
        else:
            yield from self.query_data_model_identifiers_from_database(context)


    def pre_execute(self, context):
        """
        Create task instances for all the data model identifiers. 
        
        :param context:
            The Airflow context dictionary.
        """

        args = (context["dag"].dag_id, context["task"].task_id, context["run_id"])

        # Get parameters from the parent class initialisation that should also be stored.
        common_task_parameters = self.common_task_parameters()

        pks = []
        for data_model_identifiers in self.data_model_identifiers(context):
            # This order here is important if common_task_parameters['release'] = None,
            # and the release has been inferred from the execution date.
            parameters = { **common_task_parameters, **data_model_identifiers }
            instance = create_task_instance(*args, parameters)
            pks.append(instance.pk)
        
        if not pks:
            raise AirflowSkipException("No data model identifiers found for this time period.")
        
        self.pks = pks
        return None
        

    def prepare_data(self):
        """
        A generator that yields the task instance, the path of the input data product, the 
        spectrum, and the modified spectrum (after applying any spectrum callbacks).
        """
        yield from prepare_data(self.pks)
