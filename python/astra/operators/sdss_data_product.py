import numpy as np
import inspect
from datetime import datetime
from airflow.exceptions import AirflowSkipException

from astra.database.utils import create_task_instance
from astra.operators.base import AstraOperator
from astra.operators.utils import (prepare_data, parse_as_mjd, callable_to_string)
from astra.utils import log

from sdss_access import SDSSPath

class DataProductOperator(AstraOperator):

    """
    A base operator for performing work on SDSS data products.
    """

    def common_task_parameters(
            self, 
            ignore=("self", "parameters", "args", "kwargs", "slurm_kwargs"),
            callable_keys=("python_callable", "spectrum_callback"),
            ignore_keywords_with_leading_underscores=True,
            ignore_arguments_with_nones_and_default_values=True,
        ):
        """
        Return a dictionary of keyword arguments that should be recorded as parameters for
        task instances generated by this operator. This will include:
        
            - all named keyword arguments that were initiated with the class, 
            - including those keyword arguments from parent classes,
            - and any parameters passed through `bash_command_kwargs`

        :param ignore:
            keyword names to ignore
        :type ignore: tuple of strings
        :param ignore_keywords_with_leading_underscores:
            ignore keywords with leading underscores (e.g., '_data_model_identifiers') (default: True)
        type ignore_keywords_with_leading_underscores: bool
        :param ignore_arguments_with_nones_and_default_values:
            ignore arguments where the value given is the default value (e.g., None).
        :type ignore_arguments_with_nones_and_default_values: bool

        :returns:
            A dictionary of keyword arguments to include as task parameters.
        """

        common_task_parameters = {}
        common_task_parameters.update(self.parameters)
        mros = type(self).mro()
        
        for mro in mros[:1 + mros.index(AstraOperator)]:
            signature = inspect.signature(mro.__init__)
            init_parameters = signature.parameters
            if ignore_keywords_with_leading_underscores:
                init_parameters = [pn for pn in init_parameters if not pn.startswith("_")]
            
            parameters = set(init_parameters).difference(ignore)
            for parameter in parameters:
                if ignore_arguments_with_nones_and_default_values \
                and getattr(self, parameter) is None \
                and signature.parameters[parameter].default == getattr(self, parameter):
                    continue
                common_task_parameters.setdefault(parameter, getattr(self, parameter))

        # Ensure the callables can be serialized.
        if callable_keys is not None:
            for key in callable_keys:
                if key in common_task_parameters:
                    common_task_parameters[key] = callable_to_string(common_task_parameters[key])

        return common_task_parameters


    def query_data_model_identifiers_from_database(self, context):
        """
        Query the SDSS database for ApStar data model identifiers.

        :param context:
            The Airflow DAG execution context.
        """ 
        if self.release is not None:
            releases = [self.release]
        else:
            releases = self.infer_releases(context)
        
        mjd_start = parse_as_mjd(context["prev_ds"])
        mjd_end = parse_as_mjd(context["ds"])

        for release in releases:
            if release.upper() == "DR16":
                yield from self.query_sdss4_dr16_data_model_identifiers_from_database(mjd_start, mjd_end)
            elif release.upper() == "SDSS5":
                yield from self.query_sdss5_data_model_identifiers_from_database(mjd_start, mjd_end)
            else:
                log.warning(f"Don't know how to query for data model identifiers for release '{release}'!")
                continue
    

    def data_model_identifiers(self, context):
        """ 
        Yield the data model identifiers found by this operator.
        
        :param context:
            The Airflow context dictionary.
        """

        if self._data_model_identifiers is not None:
            log.warning("Using data model identifiers specified by _data_model_identifiers. "
                        "Ignoring the DAG execution context! Use this at your own risk!")

            sources = self._data_model_identifiers
            if callable(sources):
                sources = sources()

            yield from fulfil_defaults_for_data_model_identifiers(sources, context)
        else:
            yield from self.query_data_model_identifiers_from_database(context)


    def infer_releases(self, context):
        """
        Infer the SDSS release(s) to use based on the execution context.

        :param context:
            The Airflow context dictionary.
        """
        releases = infer_releases(context["ds"], context["next_ds"])
        log.info(f"Between {context['ds']} and {context['next_ds']} the relevant SDSS releases are {releases}")
        return releases


    def pre_execute(self, context):
        """
        Create task instances for all the data model identifiers. 
        
        :param context:
            The Airflow context dictionary.
        """

        args = (context["dag"].dag_id, context["task"].task_id, context["run_id"])

        # Get parameters from the parent class initialisation that should also be stored.
        common_task_parameters = self.common_task_parameters()

        pks = []
        for data_model_identifiers in self.data_model_identifiers(context):
            # This order here is important if common_task_parameters['release'] = None,
            # and the release has been inferred from the execution date.
            parameters = { **common_task_parameters, **data_model_identifiers }
            instance = create_task_instance(*args, parameters)
            pks.append(instance.pk)
        
        if not pks:
            raise AirflowSkipException("No data model identifiers found for this time period.")
        
        self.pks = pks
        return None
        

    def prepare_data(self):
        """
        A generator that yields the task instance, the path of the input data product, the 
        spectrum, and the modified spectrum (after applying any spectrum callbacks).
        """
        yield from prepare_data(self.pks)



def infer_releases(execution_date, next_execution_date):
    """
    Infer the SDSS release(s) to use based on the execution context.

    SDSS-IV APOGEE DR16 includes observations between 2011-08-31 and 2018-08-28.
    SDSS-IV APOGEE DR17 includes observations between 2011-08-31 and 2021-01-21.
    SDSS-V APOGEE observations took place from 2020-10-24.
    
    :param context:
        The Airflow context dictionary.
    
    :returns:
        Either 'sdss5' or 'DR17' based on the execution date.
    """
    as_dt = lambda date: datetime.strptime(date, "%Y-%m-%d") if isinstance(date, str) else date

    execution_date = as_dt(execution_date)
    next_execution_date = as_dt(next_execution_date)

    known_releases = [
        ("DR16", "2011-08-31", "2018-08-28"),
        ("DR17", "2011-08-31", "2021-01-21"),
        ("sdss5", "2020-10-24", "2030-01-01") # Ambitious?
    ]

    releases = []
    for release, start_date, end_date in known_releases:
        start_date, end_date = list(map(as_dt, (start_date, end_date)))
        
        overlap = min(end_date - execution_date, next_execution_date - start_date)
        if overlap.days > 0:
            releases.append(release)
    
    return tuple(releases)


def healpix(obj, nside=128):
    """
    Return the healpix number given the object name.
    
    :param obj:
        The name of the object in form '2M00034301-7717269'.
    :type obj: str
    :param nside:
        The number of sides in the healpix (default: 128).
    :type nside: int
    """
    
    # The logic for this function was copied directly from the sdss/apogee_drp repository, and
    # written by David Nidever.

    # apogeetarget/pro/make_2mass_style_id.pro makes these
    # APG-Jhhmmss[.]ssÂ±ddmmss[.]s
    # http://www.ipac.caltech.edu/2mass/releases/allsky/doc/sec1_8a.html

    # Parse 2MASS-style name
    #  2M00034301-7717269
    name = obj[-16:]  # remove any prefix by counting from the end  
    # RA: 00034301 = 00h 03m 43.02s
    ra = np.float64(name[0:2]) + np.float64(name[2:4])/60. + np.float64(name[4:8])/100./3600.
    ra *= 15     # convert to degrees
    # DEC: -7717269 = -71d 17m 26.9s
    dec = np.float64(name[9:11]) + np.float64(name[11:13])/60. + np.float64(name[13:])/10./3600.
    dec *= np.float(name[8]+'1')  # dec sign

    import healpy as hp
    return hp.ang2pix(nside, ra, dec, lonlat=True)




def fulfil_defaults_for_data_model_identifiers(
        data_model_identifiers, 
        context
    ):
    """
    Intelligently set default entries for partially specified data model identifiers.
    
    :param data_model_identifiers:
        An list (or iterable) of dictionaries, where each dictionary contains keyword arguments
        to specify a data model product.

    :param context:
        The Airflow context dictionary. This is only used to infer the 'release' context,
        if it is not given, based on the execution date.

    :returns:
        A list of data model identifiers, where all required parameters are provided.
    
    :raises RuntimeError:
        If all data model identifiers could not be fulfilled.
    """

    try:
        releases = infer_releases(context["ds"], context["next_ds"])
    except:
        log.exception(f"Could not infer release from context {context}")
        default_release = None 
    else:
        # Take the 'most recent' release.
        default_release = releases[-1]

    trees = {}

    defaults = {
        "sdss5": {
            "apStar": {
                "apstar": "stars",
                "apred": "daily",
                "telescope": lambda obj, **_: "apo25m" if "+" in obj else "lco25m",
                "healpix": lambda obj, **_: str(healpix(obj)),
            }
        }
    }
    
    for dmi in data_model_identifiers:

        try:
            filetype = dmi["filetype"]
        except KeyError:
            raise KeyError(f"no filetype given for data model identifiers {dmi} "
                           f"-- set 'filetype': 'full' and use 'full': <PATH> to set explicit path")
        except:
            raise TypeError(f"data model identifiers must be dict-like object (not {type(dmi)}: {dmi}")
        
        source = dmi.copy()
        release = source.setdefault("release", default_release)
        
        try:
            tree = trees[release]
        except KeyError:
            trees[release] = tree = SDSSPath(release=release)
        
        missing_keys = set(tree.lookup_keys(filetype)).difference(dmi)
        for missing_key in missing_keys:
            try:
                default = defaults[release][filetype][missing_key]
            except KeyError:
                raise RuntimeError(f"no default function found for {missing_key} for {release} / {filetype}")
            
            if callable(default):
                default = default(**source)

            log.warning(f"Filling '{missing_key}' with default value '{default}' for {source}")
            source[missing_key] = default
        
        yield source
