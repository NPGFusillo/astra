\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

% page and document setup
\renewcommand{\twocolumngrid}{}
\addtolength{\topmargin}{-0.35in}
\addtolength{\textheight}{0.6in}
\setlength{\parindent}{3.5ex}
\renewcommand{\paragraph}[1]{\medskip\par\noindent\textbf{#1}~---}

% figure setup
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{shadows}
\definecolor{captiongray}{HTML}{555555}
\mdfsetup{%
  innertopmargin=2ex,
  innerbottommargin=1.8ex,
  linecolor=captiongray,
  linewidth=0.5pt,
  roundcorner=1pt,
  shadow=false,
}
\newlength{\figurewidth}
\setlength{\figurewidth}{0.75\textwidth}

% text macros
\shorttitle{Astra}
\shortauthors{Casey et al.}
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\newcommand{\astra}{\texttt{Astra}}
\newcommand{\Astra}{\astra}
\newcommand{\pipeline}[1]{\texttt{#1}}

\newcommand{\APOGEENet}{\pipeline{APOGEENet}}
\newcommand{\BOSSNet}{\pipeline{BOSSNet}}
\newcommand{\LineForest}{\pipeline{LineForest}}

\newcommand{\pytorch}{PyTorch}


% math macros
\newcommand{\todo}[1]{\textcolor{orange}{#1}}
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\mps}{\unit{m\,s^{-1}}}
\newcommand{\kmps}{\unit{km\,s^{-1}}}

\newcommand{\teff}{T_\mathrm{eff}}
\newcommand{\logg}{\log_{10}(g)}
\newcommand{\mh}{[\mathrm{M/H}]}


\sloppy\sloppypar\raggedbottom\frenchspacing
\begin{document}

%\title{\Huge Stellar parameter determination for the Milky Way Mapper (SDSS)}
%\title{\Huge Data analysis strategy for the Milky Way Mapper (SDSS)}
%\title{Inferring stellar parameters and chemical abundances for the Sloan Digital Sky Survey's Milky Way Mapper}
\title{Inferring stellar parameters and chemical abundances from the Milky Way Mapper in the Sloan Digital Sky Survey}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\affiliation{School of Physics \& Astronomy, Monash University}
\affiliation{Centre of Excellence for Astrophysics in Three Dimensions (ASTRO-3D)}

\author{others}


\begin{abstract}
\noindent The fifth generation of the Sloan Digital Sky Survey (SDSS) benefits from a panoptic view of the galaxy from both hemispheres.
The Milky Way Mapper is one of the three Mappers of SDSS-V, where it is using the BOSS (optical) and APOGEE (infrared) spectrographs to acquire millions of spectra across the entire stellar life cycle: from pre-main-sequence stars to stellar cinders.
The broader range of spectral types introduces more data analysis requirements than earlier surveys using the BOSS or APOGEE instruments.
\todo{Something about systematic differences in stellar spectra.}
Here we describe how we use \todo{X} analysis pipelines to infer stellar parameters and chemical abundances from Milky Way Mapper spectra, and the strategy to select one set of pipeline measurements per spectrum.
While this process adds complexity to the analysis, it ensures we must address systematic differences between pipelines.
\todo{Some closing remark.}
\end{abstract}

\keywords{God's --- Work}


\section*{}\clearpage
\section{Introduction} \label{sec:intro}
The Sloan Digital Sky Survey (SDSS) is now in it's fifth generation, with each seeking to address new questions in astrophysics.
The current generation is organised by three so-called Mapper programs -- Black Hole Mapper, Local Volume Mapper, and Milky Way Mapper -- for which an overview can be found in \citet{kollmeier}.
The Black Hole Mapper and Milky Way Mapper share the 2.5 meter telescope at Apache Point Observatory, and the 2.5 meter du Pont telescope at Las Campanas Observatory.
Both telescopes are equipped with nearly identical setups. A new robotic fibre positioner (FPS) system is used to configure up to 1500 fibers across a field of view, replacing the human-plugged plates that were extensively used in SDSS-IV.
The fibers are then fed to two spectrographs at each telescope, with 500 fibers fed to the BOSS low-resolution optical spectrograph, and 298 fibers fed to the APOGEE high-resolution infrared spectrograph.\\

\todo{A paragraph about scientific justification for MWM}\\

The scientific objectives encompassed by the Milky Way Mapper require that we observe a wider range of spectral types than red giant branch stars, which composed most of the objects in the SDSS-IV APOGEE survey.

The data analysis approach we adopt in MWM is different to all other large stellar spectroscopic surveys.
Instead of using a single analysis pipeline \citep{galah}, or combining results from multiple pipelines \citep{ges}, we chose to use multiple pipelines with the intent to report only one set of those results per spectrum.
There are two primary reasons for this strategy.\\

First, the increasing volume and quality of stellar spectra has highlighted the magnitude of systematic errors between surveys, and pipelines. While some discrepancies can be readily discounted due to spectra covering different wavelength ranges, the Gaia-ESO Survey experiment has clearly demonstrated that experienced spectroscopists can report statistically significant abundance differences even when controlling for the choice of stellar photosphere, line list, and continuum normalisation.
These comparisons, and many like them in the literature [cite], highlight that unquantified systematic uncertainties are present in spectroscopic analyses, and they have the capacity to limit the inferences that we want to make about the universe.
For these reasons, if we want to make measurements from spectra and use them to infer things about the Milky Way, then we should have a reliable estimate of the total uncertainties in those measurements.
Secondly, the large range of spectral types targeted by MWM requires multiple pipelines, as the assumptions that make an analysis suitable for M-dwarfs may not generally be applicable for white dwarfs.
While this is an extreme example, there are regions of parameter space where there is no justified specific boundary to motivate the use of one analysis method or another.\\

One advantage in adopting a structure where we expect that there will be many pipelines analysing a given star is that there is minimal burden for adding another pipeline. Contrast the psychology to a survey with a single pipeline\footnote{Perhaps that of a student}, where there is a relatively high barrier for entry for new methods to prevail. For these reasons, we hope to encourage innovation and change in how we analyse spectroscopic data (writ large), and this document reflects the current organisational structure for data analysis in MWM.\\

We briefly describe the analysis pipelines used in Section~\ref{sec:method}, where we highlight any functional changes to the existing implementation. We also describe the software that orchestrates the execution of these pipelines (\texttt{Astra}), including the common tools, \todo{etc}. In Section~\ref{ref:results} we outline the results. \todo{Blah Blah}\\

\section{Methods}\label{sec:method}

\Astra\ is the software we use to analyse MWM spectra.
It is responsible for producing science-ready data products from reduced spectra, and making those products available to the collaboration and the public.
While the primary purpose of executing all pipelines on spectra is simple in theory, some effort is required to meet the computational requirements of each pipeline, to efficiently orchestrate those analyses, and to make the results accessible.
Here we describe some of the technical framework and how things are organised, before detailing the individual analysis pipelines.


\subsection{Tasks} \label{ref:tasks}

\Astra\ is written in the Python programming language \citep{python}, but the pipelines it executes are written in a mix of Python, Julia, and FORTRAN. 
Pipelines are interfaced with \Astra\ by defining one or more `tasks': these are Python functions that might expect some input data (e.g., a spectrum) and it yields (generates) some measurements.
There are some technical requirements to properly define a task.
However, once defined, a task can be easily executed on a CPU/GPU cluster, through the command line, or in an interactive shell or notebook environment.\\

All \Astra\ tasks are `decorated' with a special Python function (\texttt{astra.task}). This has a specific technical meaning. If the function $f(x)$ is decorated with $g(...)$, then when the function $f(x)$ is called it actually executes the function $g(f, x)$, allowing $g(...)$ to modify the behaviour of the function $f(x)$.
\Astra\ does this to handle the execution of tasks and writing of results in a consistent way.
For example, the decorator can determine whether the code should be executed immediately, or queued to a compute cluster. It can parallelise the execution of the code in different ways. It can write the results from the pipeline in efficiently batched queries, as well as attach useful metadata with each result (e.g., CPU cost, joint overheads, software versions). This means that the scientist who developed the pipeline does not need to spend time on technical details related \emph{how} the pipeline is executed (e.g., parallelising or scaling the code; writing SQL queries or formatting output files; or testing code in different compute environments): they can focus on ensuring the pipeline works correctly, and \Astra\ will scale it to all MWM data.\\

\subsection{Organisation} \label{ref:data-models}

\Astra\ uses a database to store information about astronomical sources, spectra, and results.
An object relational mapper \citep{} is used to define the database schema, which we use to define the format and content of output data products.
Defining the database schema with an object relational mapper allows us to add field-specific information which would otherwise be impossible in modern database engines.
For example, every column definition includes an explanatory note (from a common glossary, see Appendix~\ref{appendix:glossary}) that describes the value being stored, as well as the relevant units. Enforcing a glossary helps to ensure consistent terminology from results reported by different pipelines. 
Those units and descriptions that are defined in the data model are used to populate final data products for the entire collaboration: the explanatory fields are added to the comments of output files, and automatically populate the data model documentation on the SDSS webpage. \\

We use a unique spectrum identifier (\texttt{spectrum\_pk}) that might refer to any type of spectrum (e.g., an APOGEE visit, a combined BOSS spectrum, etc). Every kind of spectrum has an associated table with the columns necessary to read the spectrum, and relevant metadata.
While some of this information is also stored in the SDSS-V central database (e.g., targeting, catalog, or reduction tables),
duplicating it into \Astra's schema allows for \Astra\ to be executed independent of the SDSS-V database.\\

There is at least one table for the results of each pipeline, and those tables include a foreign key that references the unique spectrum (or source) analysed. Summary tables of pipeline results are routinely created by joining: the pipeline results table; the relevant spectrum table (which includes all spectrum metadata); and the source table, which includes all relevant survey identifiers, astrometry, photometry, and targeting information.


\subsection{Migration}

\Astra's database is designed to be decoupled from the central SDSS database to allow for local development and testing.
For this reason, information about new spectra must be explicitly migrated (copied) into \Astra's database schema.
This occurs daily. For each new spectrum, \Astra\ ensures that it has all the requisite keywords to locate the file that stores that spectrum on disk. A database record is created for the spectrum where it is given a unique spectrum primary key (\texttt{spectrum\_pk}), regardless of what \emph{kind} of spectrum it is (e.g., APOGEE visit -- \texttt{apVisit}, BOSS \texttt{specLite}, etc). Given a spectrum primary key \Astra\ can resolve the type of spectrum it is, read it from disk, and access any metadata about the astronomical source. This provides a clean access pathway such that individual pipelines to build upon. The spectrum tables include  metadata from the image headers (or data reduction outputs), as well as some computed quantities that were requested by the collaboration (e.g., barycentric velocity, fraction of night time at the observation mid-point).\\

If the spectrum being ingested is a so-called \emph{derivative} product (e.g., a spectrum that is combined from many individual visits), then \Astra\ parses the header information of that spectrum to find those individual visits and ingest them too. The individual visits and the derivative product are then linked in the database.

The next step in the ingestion process is to link that spectrum to a unique source. This procedure is more stable now with the introduction of SDSS identifiers, as \Astra\ would need to ingest and seamlessly link spectra from current and former SDSS generations, across different telescopes, and catalog identifiers.
%Often this is not a trivial procedure, as \Astra\ needs to ingest and seamlessly link spectra from current and former SDSS generations, across different telescopes and instruments, and catalog identifiers. 
For SDSS-V spectra we use the catalog identifier specified for the observation.\footnote{If no catalog identifier is available, we use the Gaia source identifier in the image headers, or the 2MASS identifier for SDSS-IV data. In both cases we use this external identifier to find the SDSS-V catalog entry, and then perform all subsequent cross-matches.} For good technical reasons, an astronomical source can have multiple catalog identifiers: some across different targeting versions, or some within the same targeting version. However, the same astronomical source might also have two SDSS identifiers (\texttt{sdss\_id}). In such cases we take the smaller value as the representative SDSS identifier, and we record all linked catalog identifiers that \Astra\ could find across different targeting versions.

If the SDSS identifier is not already present in the \Astra\ database then a new record will be added to the \texttt{Source} table. This table has one entry per astronomical source, and includes relevant internal or external identifiers, as well as astrometry, photometry, and metadata from other surveys. The primary key is defined by a \texttt{source\_pk} column, and the SDSS identifier (\texttt{sdss\_id}) is just one optional identifier. This allows \Astra\ to analyze spectra for sources that do not necessarily have a SDSS identifier (e.g., the Sun). Later in the ingestion process, any sources with missing information will be efficiently filled in bulk so as to avoid individual queries when spectra are ingested. 

The auxillary data that are added to \Astra's source table includes: HEALPix, astrometry and photometry from Gaia, TIC (v8) identifiers, TWOMASS photometry, unWISE photometry, GLIMPSE photometry, stellar parameters from Gaia XP spectra, Bailer-Jones distances, reddening estimates (by numerous methods, see Section~\ref{sec:dust}), the number of sources that are nearby on the sky, and all SDSS-V carton assignments. There is a benefit to having all carton assignments in a format that can be easily queried, but we wanted to avoid duplicating the many-to-many relationship tables that currently describe the carton assignments in SDSS-V. For these reasons, \Astra\ stores all carton assignments in a single column as a bytearray. This allows for a flexible (expanding) storage of all carton assignments as a sequence of integers. This  serves as a representation of the carton assignments so that pipelines can efficiently query sources by cartons (or other attribute).


Analysis results from analysis pipelines are stored in their own database table. Each result table includes a reference to the spectrum primary key, which can be used to link to the unique source and it's associated metadata. The output summary files that SDSS users have become accustomed to are simply a database join across source, spectrum, and result tables, possibly with restrictions by observation date and a version of the data reduction pipeline. For these reasons, every column in an \Astra\ table has an associated human-readable definition and units. The units and definition are populated into the \Astra\ output tables as comments, ensuring consistent definitions of column names and units across all output data products. The complete data model glossary is listed in Appendix~\ref{appendix:glossary}.



%\subsection{Common tools}
%\Astra\ includes some common tooling to prevent repetitive code in each pipeline.
%This includes handlers for reading and writing SDSS spectra (in various formats), continuum normalisation routines, 


\subsection{Execution}

We use Apache Airflow \citep{} to orchestrate and execute \Astra\ tasks on MWM spectra.
This framework requires us to define directed acyclic graphs\footnote{A directed acyclic graph is not dissimilar to what an astronomer would describe as a `pipeline'.} to analyse MWM spectra.
The advantages of doing this are that Airflow can be used to analyse many millions of spectra easily, where it responsibly manages any resources (e.g., clusters, databases), will attempt to re-execute tasks that fail for sporadic issues (e.g., database connectivity), and this can be readily visualised and controlled through a web interface. 
% The disadvantages are that Airflow is a fucking shit show to work with.
%While a single directed acyclic graph is sufficient for processing MWM spectra, during in practice during 
%We use multiple directed acyclic graphs to process MWM data, primar
Executing \Astra\ tasks with Airflow is a convenience, not a necessity: the Airflow `operators' included in \Astra\ are primarily to collect reduced spectra of a particular type within some time period (e.g., APOGEE visits between now and yesterday).


\subsection{Derivative data products: mwmVisit and mwmStar}


\todo{
These things have to go somewhere:
- hierarchy of data products -> what gets produced by what?
}


\subsection{Design}

- Needs to include relevant information from the data reduction pipeline, and any upstream information (e.g., radial velocities)

Why does it need to do all this? 
Because there will be multiple versions of the reduction for the data, and we want to be able to analyse all of those spectra and keep the results together in a database. Every spectrum in the database needs to be able to find the location of the spectrum on disk.

In the end, the output files that \astra\ prepares for a data release will be database exports of subsets of the data (e.g., for a specific data reduction version, range of observation dates, and telescope). 


\subsection{Pipelines}
Here we include a short description of all pipelines included in \astra. 
In general we reference the original papers of those pipelines for detailed explanations, however, we do list any modifications made to those pipelines.
All pipelines have been modified to some degree. In some cases only basic refactoring took place to make the functions compatible with how other pipelines are executed. Some pipelines perform some kind of functionality which we refactored because \astra\ includes that functionality as a common tool, and by refactoring the pipeline we were able to make easy tests of different choices (e.g., how continuum is modelled). In the most extreme cases, the pipeline has been re-written in it's entirety from scratch, whilst keeping the same approach.

\subsection{FERRE} \label{sec:methods-ferre}

FERRE \citep[or FERRE]{ferre} % \todo with the reverse R
is a FORTRAN tool to compare model spectra with observations.
The model spectra should be some rectilinear (evenly spaced) grid of spectra which you want to compare against models.
In the typical use case in \astra, the model grid is convolved to the expected spectral resolution and wavelength sampling of the data.
For a given observed spectrum, the best-fit model is found by interpolating a multi-dimensional grid of spectra.
For computational reasons, and to avoid the so-called `noding' effects \citep{CITE}, the grid of model spectra is stored in a compressed form.

FERRE includes some options when fitting spectra, including the initial guess, and any dimensions to be frozen. For more details see \citet{CITE}.
The continuum is optionally fit simultaneously with the stellar parameters, or can be performed before executing FERRE.
The best-fitting spectrum is found by an optimization routine (Nelder-Mead?).

%Only minimal changes were made to the FERRE code, but more substantive changes were made to the inputs that went in to FERRE (e.g., how ASPCAP is executed).
%Only minimal changes were made to the FERRE code, but more substantive changes were made to how FERRE was executed (e.g., see Section \ref{sec:aspcap-methods}).
The only changes made to FERRE were on the formats of output files. We added the \todo{input name} to output files to ensure that we could correctly order the results from all files when FERRE was being executed in parallel. 
Previously FERRE would correctly sort all files at the end of execution, but this sorting implementation was unexpectedly inefficient. 
After adding the FERRE input name to each row of the output files and the standard output, we disabled the concluding sorts.
Adding the input name to the standard output also allowed \astra\ to better track the time spent analysing per spectrum, and the joint overheads.

FERRE can be executed as a single task in \astra, but it is more often executed as just one step in the analysis.
Custom reader and writer functions to execute FERRE were added to \astra\ to handle the three ways in which FERRE is usually executed: for a coarse estimate of stellar parameters; a detailed fit of all stellar parameters; or by estimating the abundances. These populate different database tables. 

\subsection{ASPCAP} \label{sec:methods-aspcap}

ASPCAP is the APOGEE Stellar Parameter and Chemical Abundances Pipeline \citep{aspcap} that was developed and used by the APOGEE survey in SDSS-IV.\footnote{Those unfamiliar to SDSS may appreciate knowing that there is an APOGEE instrument \emph{and} an APOGEE survey. If not explicitly specified, when referring to APOGEE we mean the APOGEE instrument.}


The ASPCAP and FERRE codes are often described as being synonymous, but there is a clear delineation between the two. FERRE \citep[and Section \ref{sec:methods-ferre}][]{ferre} performs $\chi^2$ minimisation between an observed spectrum and some model grid. FERRE is not usually used to simultaneously estimate both the stellar parameters and detailed chemical abundances. ASPCAP refers to the procedure to estimate stellar parameters and chemical abundances of a star by calling FERRE multiple times.

In the context of SDSS, only APOGEE spectra are analysed with ASPCAP, but ASPCAP has been used for other surveys \citep{who}. 


ASPCAP is probably the most comprehensive pipeline that is integrated in \Astra, and has gone through the most refactoring. Here we will provide a slightly longer-than-average description of ASPCAP so that we can highlight differences in the ASPCAP version in \Astra.

The most notable difference in the ASPCAP implementation in \Astra\ is that it has been entirely re-written in Python. The ASPCAP implementation used in SDSS-IV was written in IDL. While \Astra\ could have executed the existing IDL pipeline, there were numerous technical reasons that motivated a rewrite. Institutional memory of the existing IDL pipeline was limited, and the existing implementation planned executions on a per-plate basis -- a concept which was to change with the new fiber positioners in SDSS-V. Some re-structering of the code would be required to handle changes in file paths, data models, etc. Having a Python based version also gave more flexibility to experiment and improve the code. 


ASPCAP executes FERRE many times using different grids, where each grid might cover some subset of stellar parameter space. The hot star grid is convolved to a representative spectral resolution of 22,500, but the other grids are convolved with four different line spread functions. The fiber used to observe the target (or the mean fiber from multiple observations) determines which of the four convolved grids will be used.

There are three sequential stages in ASPCAP: 
  the coarse stage, 
  the stellar parameter stage, and 
  the chemical abundances stage. 
At the start of the coarse stage, it is useful if there is an initial guess of the stellar parameters for soem given spectrum. In SDSS-IV, the Doppler estimate of the stellar parameters was often used as the initial guess. In SDSS-V we use the stellar parameters from \APOGEENet\ if available (for that spectrum, or alternatively for that star), and revert to the Doppler estimate if there are no \APOGEENet\ results available. A bitmask flag is recorded to indicate the source of the initial guess.

Given the initial guess, we consider whether each grid spans that position in stellar parameters. If it does-- and if the LSF matches that for the fiber, or it is the hot star grid -- then that grid will be used to estimate coarse stellar parameters for the given spectrum. Because the grids used will overlap in stellar parameters \citep{dr17-grids}, it is frequent that the same spectrum will be analysed twice during the coarse stellar parameter stage. 

In the special case where no initial guess is available, \emph{all} grids with a suitable LSF will be used in the coarse parameter stage. 


Multiple dimensions of the FERRE grid are held fixed during the coarse stellar parameter stage. For main-sequence grids, we \todo{fix [C/M], [N/M], [$\alpha$/M] = 0}. \todo{Microturbulence is also kept fixed based on the following relation.}. For giant star grids, we fix \todo{X, Y, and Z}.

When multiple FERRE executions have taken place for a single spectrum in the coarse stage, the best set of coarse stellar parameters is found by comparing the (penalized) $\chi^2$ of different solutions. The penalized value takes the original $\chi^2$ value and multiplies it if specific conditions are met. For example, if the effective temperature or surface gravity is within the last node of a grid edge, it is multiplied by 5. If those parameters are within 1/8th of the edge grid spacing, it is multiplied by 10. If FERRE fails to report an effecive temperature or surface gravity, it is multiplied by 20. The final condition is for stars with FERRE-reported $\teff < 3900$\,K and if the GK-type grid was used. In that condition, the $\chi^2$ is penalized by 10. 


%The results from each FERRE execution are stored in the \Astra\ database, but these are not usually acces

\subsubsection{Stellar Parameters Stage}

All stellar parameters are allowed to vary in this stage. The best result from the coarse stage is used as an initial guess in the stellar parameter stage, and the FERRE grid where that coarse stage was reported is taken as the grid to use in the stellar parameter stage. 

Like in SDSS-IV, a pseudo-continuum normalisation is performed before the stellar parameter stage. We take the best-fitting result from the coarse stage and divide the model spectrum by the observed one on a per-chip basis. We interpolate over bad pixels (e.g., extremely low, high, or non-finite flux) and compute a median filter of the `ratio spectrum' with a filter width of 151 pixels. This matches the approach used in SDSS-IV, but the implementation can lead to slightly different results. The median filter of the ratio spectrum is then assumed to be the continuum for the stellar parameters stage. 

The biggest source of differences in stellar parameters between the ASPCAP implementation used in SDSS-IV, and the Python version implemented in \Astra, likely arise from different choices of initial guesses (e.g., using \APOGEENet\ instead of Doppler) and minor adjustments to how the median filter is applied for the pre-continuum step.

\subsubsection{Chemical abundances stage}


In the chemical abundances stage of ASPCAP the stellar parameters are held fixed to the values found in the earlier stage, and the abundances are determined by allowing metallicity-sensitive dimension to vary. The technical details about how this is done is the same as in SDSS-IV (e.g., through \texttt{TTIES} keywords and freezing parameters). 

In SDSS-IV the median filtering procedure is also applied before the chemical abundances stage (using the best-fitting result from the stellar parameter stage). The \Astra\ implementation does the same thing, but it does not allow FERRE to adjust the continuum during the chemical abundances stage. The logic is that some chemical abundances are only inferred from a very small number of pixels, and if we also allow a fourth order polynomial to vary while trying to fit a chemical abundance to a few pixels, the problem will be too degenerate. For this reason, the ASPCAP version in \Astra\ will perform the pre-continuum step before the chemical abundances stage, but it will then disable any continuum-fitting options in FERRE. 


\subsubsection{ASCPAP results}

A single spectrum analysed by ASPCAP might involve 30 or more FERRE executions: a few in the coarse stage, one in the stellar parameter stage, and the remainder during the chemical abundances stage.

The results from each FERRE execution are available in the \Astra\ database, but are not usually made available to users through output tables. Most users want the stellar parameters estimated during the stellar parameter stage, and a summary of abundances from the executions made during the chemical abundances stage.

For these reasons, a final task queries the results from the different FERRE tables and creates a usable table of stellar parameters and chemical abundances, with primary keys linking to the individual tasks. 

\todo{Corrections made. What elements do we incldue this time round?}


\subsection{The Payne} \label{sec:methods-the-payne}

\subsection{The Cannon} \label{sec:methods-the-cannon}

The Cannon \citep{Ness,thecannon} is a data-driven method for estimating stellar labels ($\teff$, $\logg$, chemical abundances,and more). The implementation of The Cannon in \Astra\ is suitable for use with either APOGEE or BOSS spectra. Since all the spectra in the \Astra\ database are linked to other survey identifiers, one can easily train a new model with The Cannon that uses published labels from a previous SDSS data release. 

The Cannon implementation in \Astra\ is trained by using the coordinate descent algorithm, which differs from how previous implementations were optimised. Coordinate descent seems to be efficient for small or large models, and the CPU cost to train a model does not vary substantially with increasing regularisation. In practice, previous implementations seemed to have a higher CPU cost as the regularisation strength grew. Coordinate descent also seems to produce sparser models with more interpretable flux derivatives.

\todo{Fixing s2 after the fact}

Since highly regularised models can be trained efficiently via coordinate descent, the new implementation of The Cannon in \astra\ allows for grid searches in the regularization strength $\lambda$. The default grid search behaviour is to place uniform-in-log values of $\Lambda$ between $10^{-8}$ and $10^{-1}$, and train a model at each regularization strength. We then compute the $\chi^2$ of the validation set (distinct from the training set) by predicting the spectra of the validation set -- with the validation labels -- and computing the $\chi^2$ difference. Like \citet{Casey} and elsewhere, we find there is a minima where an increasing regularization strength produces a sparser model that also makes better predictions of spectra. We take the minima of this to be the regularization strength. 



%\subsection{The Classifier} \label{sec:methods-the-classifier}


\subsection{Snow White} \label{sec:methods-snow-white}

The `Snow White' pipeline exclusively analyses white dwarf spectra, which are usually observed with the BOSS 
spectrograph. % todo: how many white dwarfs observed with BOSS compared to APOGEE or both?
`Snow White' measures the equivalent width of \todo{N} lines, and uses a pre-trained random forest \citep{RF} to classify white dwarfs into their sub-types. For white dwarfs that are classified as primarily DA-type, the stellar parameters are fit by interpolating a grid of \citep{who} model spectra and minimising the $\chi^2$ difference between the model and data.

% todos here
- what model grid is used?
- which lines are used?
- continuum normalisation?
- linear interpolation? in how many dimensions
- PCA compression?
- what are the different classification methods and meanings? (there is some structure to this based on the types and probabilistic classifications)

\subsection{MDwarfType} \label{sec:methods-m-dwarf-type}

The `MDwarfType' pipeline classifies M-dwarfs based on an empirical library of \todo{N} spectra of late K- and M-type stars. The empirical library was constructed by Lepine et al. from observations taken at \todo{where}. The \todo{N} spectra have been continuum-rectified by taking the \todo{mean flux between X and Y}.

Most M-dwarf stars in Milky Way Mapper are observed by BOSS (\todo{percent}). We normalise the BOSS observations in the same way that the empirical library has been normalised. We compute the $\chi^2$ difference between the observed spectrum and all \todo{N} templates, and report the best-fitting $\chi^2$ value and template type. The template type includes the spectral classification, as well as a so-called `sub-type' which is used as a coarse proxy for an M-dwarf metallicity.

The `MDwarfType` pipeline is only executed on main-sequence type stars that have been assigned to a carton that is studying M-dwarf stars. That list includes \todo{here}. \todo{Any additional photometric cut that we put on these?}.

% todo citation literature for all the Lepine group stuff.

% todo: a figure of the empirical spectra?

\subsection{HotPayne} \label{sec:methods-hot-payne}

\subsection{APOGEENet} \label{sec:methods-apogee-net}

\todo{We have versions 2 and 3 in astra}

\APOGEENet\ is a convolutional neural network for estimating stellar parameters ($\teff$, $\logg$, $\mh$) from APOGEE spectra \citep{apogeenet}. Previous versions of \APOGEENet\ required infrared photometry and astrometry (e.g., parallax to compute absolute magnitudes), and in those versions the results were sensitive to missing data. The current version of \APOGEENet\ is version 3, which does not require auxillary photometry. 

\APOGEENet\ is trained on high quality labels from SDSS-IV DR17 \citep{dr17}. 

It is implemented in \pytorch\ and is most efficiently executed on a graphics processing unit (GPU). \Astra\ can readily orchestrate tasks across CPU or GPU clusters, and the default is for \APOGEENet\ to be executed on shared GPU infrastructure at Utah.

\todo{- network structure}

Only minor technical changes were made to the \APOGEENet\ pipeline in \Astra. A task function was written to execute spectra, and we added a load balancer to better handle CPU/GPU bottlenecks.

\subsection{BOSSNet} \label{sec:methods-boss-net}

\BOSSNet\ is a convolutional neural network for estimating stellar parameters ($\teff$, $\logg$, $\mh$) from BOSS spectra \citep{bossnet}. \BOSSNet\ is the natural extension to optical spectra from the same group that developed \APOGEENet. Like the current version of \APOGEENet, no auxillary photometry or astrometry is required by \BOSSNet.

\todo{
- network structure
- training set for bossnet
- testing set
}

\subsection{AstroNN} \label{sec:methods-astro-nn}

\newcommand{\astronn}{\texttt{AstroNN}}

\astronn\ estimates the astrophysical labels given some stellar spectrum \citep{leung}.


We used a PyTorch representation of the network due to TensorFlow version incompatibilities. No other substantive changes were made. 


\subsection{CORV} \label{sec:methods-corv}

The COmpact Radial Velocity (\texttt{corv}) code computes radial velocities for compact objects. Presently it only computes radial velocities for sources that the \texttt{SnowWhite} pipeline has classified as a DA-type white dwarf, and \texttt{SnowWhite} is only executed on sources that were assigned to a white dwarf carton (\todo{name them}).

CORV fits the hydrogen lines of DA-type white dwarfs either by linearly interpolating spectra from an existing grid, or by directly fitting voigt profiles to the hydrogen lines. In data release 19, the \todo{voigt profile results} are used. The reported results include radial velocity (and associated error), effective temperature, surface gravity, and other metadata. No substantive changes were made to this code; the minimal amount of refactoring necessary was done.

\subsection{LineForest} \label{sec:methods-line-forest}

\LineForest\ measures the integrated strength of up to 52 strong transitions in the BOSS wavelength region. \LineForest\ is executed on the per-MJD BOSS visits, and on the combined BOSS spectrum of anything that looks like a star (see Targeting section). A complete description of \LineForest\ can be foudn in \citet{somewhere}. The transitions include hydrogen line (Balmer H1-17, Paschen 7-17), strong calcium lines (Ca H, K, Ca II triplet), He I and II lines, as well as N, S, Fe, O, and Li. A pre-set window is used around each line to determine the strength of the line relative to the local continuum (usually 50 or 200\,\AA). The equivalent width of each transition is reported, as well as metrics related to the detection significance, and percentiles of the equivalent width. 


\subsection{SLAM} \label{sec:methods-slam}

The Stellar Labels Machine (SLAM) is a \todo{...}.

It only considers spectra between 6417 and 8910\,\AA. 

Using a model trained on ASPCAP labels from DR16.

The Slam performs an initial normalization of the BOSS spectrum and re-samples it to the wavelength of the model grid using a cubic spline. An initial estimate of the stellar parameters is then found by a $\chi^2$ match to the grid, which is then refined by optimizing the stellar parameters. This version of the Slam is trained on ASPCAP labels from DR16, and only $\teff$ and $\mh$ are reported (with their associated uncertainties and covariances).


In \todo{IPL-3}, the Slam was executed on any source that was assigned to either the solar neighbourhood census carton (\texttt{mwm\_snc}) or the young stellar object carton (\texttt{mwm\_yso}), or if it met the following criteria:
\begin{itemize}
	\item has a reported G-band magnitude from Gaia DR3;
	\item has a reported RP-band magnitude from Gaia DR3;
	\item has a non-negative parallax in Gaia DR3;
	\item has $(G - RP) > 0.56$; and
	\item has an absolute G-band magnitude greater than 5.553
\end{itemize}


%After the data reduction pipelines are complete, 
%  is the code that does everything from data reduction to data releases. It orchestrates
%analysis of all MWM optical and infrared spectra, including over a dozen different spectral
%analysis pipelines. This includes statistical consistency checks, noise models, pouring in
%metadata from all different places, etc. It creates all the data products for the MWM data
%releases.
%Multiple analysis pipelines are required to span the wide range of spectral types targeted by
%MWM. Astra is the software that orchestrates how stellar parameters and chemical
%abundances are measured from optical and infrared spectra. This proposed technical paper
%describes the implementation, methods, and performance of Astra and those pipelines.
\section{Results}\label{sec:results}

- Refer to Don Schneider stuff

- Comments on individual pipeline results? I guess just flagging st


\section{Discussion} \label{sec:discussion}


\section{Conclusions}


% Appendix: Flags?
% Appendix: Stellar cartons?

\paragraph{Software}
\texttt{numpy} \citep{numpy} ---
\texttt{matplotlib} \citep{matplotlib}.

\paragraph{Acknowledgements}
It is a pleasure to thank
-- people
for valuable discussions and input.

\begin{thebibliography}{dummy}
\bibitem[Kelson(2003)]{kelson} Kelson, D.~D.\ 2003, \pasp, 115, 688. doi:10.1086/375502
\end{thebibliography}

\end{document}
