## Astra and SDSS-V

In the Milky Way Mapper (MWM) of SDSS-V, we use Astra to analyse spectroscopic data. The data are hosted on the [Science Archive Server](#), and there is a dedicated cluster for performing these analyses. We use [Airflow](#) to schedule and orchestrate analysis tasks for the MWM. In Airflow, we define a [directed acyclic graph (DAG)](developer/dags) that performs tasks in a pre-defined sequence, and each step can have logic that re-directs the tasks that need executing.


Every DAG has a start date, a pre-defined execution schedule, and an optional end date. Once it is created and turned on, it will be executed automatically by Airflow, with the execution time depending on the schedule: hourly, daily, weekly, yearly, or perhaps just once-off. That means when a DAG is executed, it is usually only working on data that is relevant for that execution period. In the context of SDSS-V, this could mean _'data that was observed in the last day'_, or it might mean _'data that were reduced in the last week'_. It depends on the DAG execution schedule, and what gets executed in the DAG.

Together, Airflow and Astra manage the scheduling and execution of all tasks. For example, some tasks might be very fast to complete and will be executed by Airflow as a Python executable. Other tasks might require large amounts of compute time, and need some book-keeping both before and after the task is executed. In this situation, the book-keeping is done within the task, and the expensive part is submitted as a job to the supercomputing cluster instead. The process is similar for analysis tasks that benefit from specialised hardware (e.g., GPUs): Airflow will schedule the order of the tasks to be completed; Astra will manage all the book-keeping and execute the expensive analysis on a GPU.

If we think of a task is the smallest unit of work, then you might imagine a task might be something like: _measure the strength of an absorption line in a spectrum_. It's a simple task that requires an input data product (a spectrum), has a few parameters (e.g., wavelength, optimisation settings), and produces a few output values. If you wanted to run this task on *every* SDSS-V spectrum then it would mean looping over all SDSS-V data products, creating a task for each spectrum. That's where [operators](developer/operators) come in. Operators are a concept in Airflow which you can think of as a 'task factory': it generates tasks to be executed.

Operators are necessary for running Airflow and Astra together in an efficient way. An example of an SDSS-V operator in Astra is {obj}`astra.operators.sdss.ApStarOperator`: an operator to find all ApStar files that have been created in the previous execution period. If the DAG has a weekly execution schedule, then the {obj}`astra.operators.sdss.ApStarOperator` will find and create {obj}`astra.database.astradb.DataProduct` records for every ApStar file created in the previous week.

We can pass the results of this operator down to other operators in the DAG to perform analyses on the ApStar products created in the last execution period. However, the {obj}`astra.operators.sdss.ApStarOperator` is an example of a **SDSS-V only** operator that won't work on your local computer without a bit of work. In particular, that operator requires read-access to the SDSS-V database, which can only be accessed from the SDSS systems at Utah. You can write DAGs to analyse data locally, but you need to have access to the SDSS-V data (e.g., database access and/or a mirror of the Science Archive Server) to be able to analyse SDSS-V data locally.
