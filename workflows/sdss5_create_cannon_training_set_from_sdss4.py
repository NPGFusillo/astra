import os
import numpy as np
import pickle
import astropy.table 
from tqdm import tqdm

import astra
from astra.tasks.base import BaseTask
from astra.tasks.targets import (LocalTarget, DatabaseTarget, MockTarget)
from astra.tasks.io.sdss4 import ApStarFile
from astra.tasks.continuum import Sinusoidal
from astra.tools.spectrum import Spectrum1D



class ContinuumNormalizePixelWeightedSpectrum(Sinusoidal, ApStarFile):

    """
    A pseudo-continuum normalisation task for stacked spectra in ApStarFiles that 
    uses sums of sines and cosines to model the continuum.
    """

    task_namespace = "TheCannon"

    # For training we only want to take the first spectrum, which
    # is stacked by individual pixel weighting.
    spectrum_kwds = dict(data_slice=(slice(0, 1), slice(None)))

    def requires(self):
        return ApStarFile(**self.get_common_param_kwargs(ApStarFile))


    def output(self):
        """ The outputs generated by the task. """

        if self.is_batch_mode:
            return [task.output() for task in self.get_batch_tasks()]

        path = os.path.join(
            self.output_base_dir,
            f"star/{self.telescope}/{self.field}/",
            f"apStar-{self.apred}-{self.telescope}-{self.obj}-{self.task_id}.fits"
        )
        os.makedirs(os.path.dirname(path), exist_ok=True)
    
        return LocalTarget(path)




class CreateTrainingSet(Sinusoidal):

    task_namespace = "TheCannon"

    label_names = astra.ListParameter()
    calibration_set_path = astra.Parameter()

    minimum_ivar = astra.FloatParameter(default=1e-6)
    minimum_edge_fraction = astra.FloatParameter(default=0.98)
    
    def requires(self):
        self.calibration_set, all_batch_kwds = read_calibration_set(self.calibration_set_path)
        
        common_kwds = self.get_common_param_kwargs(ContinuumNormalizePixelWeightedSpectrum)
        return ContinuumNormalizePixelWeightedSpectrum(**{ **common_kwds, **all_batch_kwds })
            

    def run(self):
        # The training set file should be a binary pickle file that contains a dictionary
        # with the following keys:
        #   wavelength: an array of shape (P, ) where P is the number of pixels
        #   flux: an array of flux values with shape (N, P) where N is the number of observed spectra and P is the number of pixels
        #   ivar: an array of inverse variance values with shape (N, P) where N is the number of observed spectra and P is the number of pixels
        #   labels: an array of shape (L, N) where L is the number of labels and N is the number observed spectra
        #   label_names: a tuple of length L that describes the names of the labels

        initial_spectrum = Spectrum1D.read(self.input()[0].path, format="SDSS APOGEE apStar")
        P = initial_spectrum.flux.size
        S = len(self.calibration_set)

        training_set = dict(
            wavelength=initial_spectrum.wavelength.value,
            label_names=self.label_names,
            labels=np.array([
                self.calibration_set[label_name] for label_name in self.label_names
            ]),
        )
        
        flux = np.ones((S, P))
        ivar = np.zeros_like(flux)
        
        for i, output in enumerate(tqdm(self.input())):
            spectrum = Spectrum1D.read(output.path, format="SDSS APOGEE apStar")
            flux[i] = spectrum.flux.value.flatten()
            ivar[i] = spectrum.uncertainty.quantity.value.flatten()

        # Check for finite-ness.
        finite = np.isfinite(flux) * np.isfinite(ivar)
        flux[~finite] = 1
        ivar[~finite] = 0

        # Check for edge effects.
        fraction = np.sum(ivar > self.minimum_ivar, axis=0) / S
        on_edge = fraction <= self.minimum_edge_fraction
        flux[:, on_edge] = 1
        ivar[:, on_edge] = 0

        training_set.update(flux=flux, ivar=ivar)
        
        with open(self.output().path, "wb") as fp:
            pickle.dump(training_set, fp)


    def output(self):
        return LocalTarget(os.path.join(
            self.output_base_dir,
            f"{self.task_id}.pkl"
        ))




if __name__ == "__main__":


    # Normally the astra.contrib.thecannon.tasks.TrainTheCannon task only requires
    # that the training set file exists (e.g., it assumes it was made somewhere else).
    # That's fine, but if the training set file doesn't exist then the TrainTheCannon
    # task will fail because it doesn't know how to create the file.

    def read_calibration_set(path, N_max=None, verbose=False):
        calibration_set = astropy.table.unique(
            astropy.table.Table.read(calibration_set_path), 
            ("TELESCOPE", "FIELD", "APOGEE_ID")
        )

        batch_kwds = {}
        for parameter_name in ApStarFile.batch_param_names():
            batch_kwds[parameter_name] = []

        for i, row in enumerate(calibration_set, start=1):
            kv_pair = {
                "apred": "r12",
                "apstar": "stars",
                "prefix": row["FILE"][:2],
                "telescope": row["TELESCOPE"],
                "field": row["FIELD"],
                "obj": row["APOGEE_ID"],
            }
            for k, v in kv_pair.items():
                batch_kwds[k].append(v)

            if N_max is not None and i >= N_max:
                break

        if N_max is not None:
            calibration_set = calibration_set[:N_max]

        if verbose:
            batch_kwds = [{k: batch_kwds[k][i] for k in batch_kwds} for i in range(len(batch_kwds["obj"]))]

        return (calibration_set, batch_kwds)


    # The allCal file does not have TEFF, LOGG, FE_H, etc.
    # (It's dumb that we have to do this.)
    calibration_set_path = "allCal-r12-l33-subset.fits"
    original_calibration_set_path = "/home/andy/data/sdss/apogeework/apogee/spectro/aspcap/r12/l33/allCal-r12-l33.fits"
    t = astropy.table.Table.read(original_calibration_set_path)
    t["TEFF"] = t["FPARAM"][:, 0]
    t["LOGG"] = t["FPARAM"][:, 1]
    t["FE_H"] = t["FPARAM"][:, 3]

    # Restrict it to some good-ish things.
    mask = (t["ASPCAPFLAG"] == 0) \
         * (t["TEFF"] < 6500) \
         * (t["SNR"] > 200)
    t = t[mask]
    t.write(calibration_set_path, overwrite=True)

    


    # Now let's get 1000 stars (each with many visits) to run analysis on
    expected, all_apStar_kwds = read_calibration_set(original_calibration_set_path, 1000)


    common_kwds = dict(
        label_names=("TEFF", "LOGG", "FE_H"),
        calibration_set_path=calibration_set_path,
        continuum_regions_path="python/astra/contrib/thecannon/etc/continuum-regions.list",
    )

    task = CreateTrainingSet(**common_kwds)

    